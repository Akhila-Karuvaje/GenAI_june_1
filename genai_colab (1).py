# -*- coding: utf-8 -*-
"""GenAI_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3Mz6ig7Z5egzmDDf-yJi3x96PROIfbv
"""

import gensim.downloader as api
model = api.load("glove-wiki-gigaword-50")
print(model.most_similar(positive=['king', 'woman'], negative=['man']))
result = model.most_similar(positive=['woman', 'king'], negative=['man'])
print("king - man + woman is closest to:", result[0])

import gensim.downloader as api
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
model = api.load("glove-wiki-gigaword-50")
words = ['computer', 'internet', 'software', 'hardware', 'keyboard', 'mouse', 'server', 'network', 'programming', 'database']
vectors = [model[word] for word in words]
pca = PCA(n_components=2)
reduced = pca.fit_transform(vectors)
input_word = 'computer'
similar_words = model.most_similar(input_word, topn=5)
print(f"Top 5 words similar to '{input_word}':")
for word, score in similar_words:
    print(f"{word}: {score:.4f}")
plt.figure(figsize=(8, 6))
for i, word in enumerate(words):
    plt.scatter(reduced[i, 0], reduced[i, 1])
    plt.annotate(word, (reduced[i, 0], reduced[i, 1]))
plt.title("PCA Visualization of Technology Word Embeddings")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.show()

from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
sentences = [
    "This is a legal document about contracts.",
    "The court will review the legal case.",
    "Medical professionals require specific training.",
    "This is a medical report about the patient."
]
tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]
model = Word2Vec(
    sentences=tokenized_sentences,vector_size=50,window=5,min_count=1,workers=4,epochs=10
)
print(model.wv.most_similar('legal'))

import gensim.downloader as api
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from nltk.tokenize import word_tokenize
import string, os
os.environ['a'] = '1'
wv = api.load("glove-wiki-gigaword-100")
tokenizer = AutoTokenizer.from_pretrained("gpt2",local_files_only=True)
model = AutoModelForCausalLM.from_pretrained("gpt2",local_files_only=True)
gen = pipeline("text-generation", model=model, tokenizer=tokenizer, framework="pt")
def gen_resp(prompt, max_length=100):
    try:
        return gen(prompt, max_length=max_length, num_return_sequences=1)[0]['generated_text']
    except Exception as e:
        print(f"Error generating response: {e}")
prompt = "Who is king."
print(f"Original Prompt: {prompt}")
key = "king"
words = word_tokenize(prompt)
res_words = []
for w in words:
    cw = w.lower().strip(string.punctuation)
    if cw == key:
        try:
            res_words.append(wv.most_similar(cw, topn=1)[0][0])
            continue
        except KeyError:
            pass
    res_words.append(w)
enriched_prompt = " ".join(res_words)
print(f"Enriched Prompt: {enriched_prompt}")
print("\nOriginal Prompt Response:")
print(gen_resp(prompt))
print("\nEnriched Prompt Response:")
print(gen_resp(enriched_prompt))

import gensim.downloader as api
from gensim.models import KeyedVectors
import random
model = api.load("glove-wiki-gigaword-100")
def generate_similar_words(seed_word, topn=10):
    if seed_word in model:
        return [word for word, _ in model.most_similar(seed_word, topn=topn)]
    else:
        return []
def create_paragraph(seed_word):
    similar_words = generate_similar_words(seed_word, topn=10)
    if not similar_words:
        return f"No similar words found for '{seed_word}'."
    random.shuffle(similar_words)
    selected_words = similar_words[:5]
    paragraph = f"In a world defined by {seed_word}, "
    paragraph += f"people found themselves surrounded by concepts like {', '.join(selected_words[:-1])}, and {selected_words[-1]}. "
    paragraph += f"These ideas shaped the way they thought, acted, and dreamed. Every step forward in their journey reflected the essence of '{seed_word}', "
    paragraph += f"bringing them closer to understanding the true meaning of {selected_words[0]}."
    return paragraph
seed = "life"
print(create_paragraph(seed))

from transformers import pipeline
sentiment_pipeline = pipeline("sentiment-analysis")
input_sentences = [
    "The new phone I bought is absolutely amazing!",
    "Worst customer service ever. I'm never coming back.",
    "The experience was average, nothing special.",
    "Fast delivery and the packaging was perfect.",
    "The product broke within two days. Very disappointed."
]
results = sentiment_pipeline(input_sentences)
print("Sentiment Analysis Results:\n")
for sentence, result in zip(input_sentences, results):
    print(f"Input Sentence: {sentence}")
    print(f"Predicted Sentiment: {result['label']}, Confidence Score: {result['score']:.2f}\n")

from transformers import pipeline
summarizer = pipeline("summarization", model="t5-small")
text = """
The Industrial Revolution, which took place from the 18th to the 19th centuries,
was a period during which predominantly agrarian, rural societies in Europe and America became industrial and urban.
Prior to the Industrial Revolution, manufacturing was often done in peopleâ€™s homes, using hand tools or basic machines.
Industrialization marked a shift to powered, special-purpose machinery, factories and mass production. The iron and textile industries,
along with the development of the steam engine, played central roles in the Industrial Revolution,
which also saw improved systems of transportation, communication and banking.
While industrialization brought about an increased volume and variety of
manufactured goods and an improved standard of living for some, it also resulted in often grim
employment and living conditions for the poor and working classes.
"""
summary = summarizer(text, max_length=60, min_length=30, do_sample=False)
print(summary[0]['summary_text'])

from pydantic import BaseModel
from typing import List, Optional
import wikipediaapi
class InstitutionDetails(BaseModel):
    founder: Optional[str]
    founded: Optional[str]
    branches: Optional[List[str]]
    number_of_employees: Optional[int]
    summary: Optional[str]
def fetch_institution_details(name: str) -> InstitutionDetails:
    page = wikipediaapi.Wikipedia(user_agent='your-user-agent', language='en').page(name)
    if not page.exists():
        raise ValueError(f"'{name}' does not exist on Wikipedia.")

    info = {
        k.strip(): v.strip()
        for line in page.text.split('\n') if ':' in line
        for k, v in [line.split(':', 1)]
    }
    return InstitutionDetails(
        founder=info.get('Founder'),
        founded=info.get('Founded'),
        branches=info.get('Branches', '').split(',') if 'Branches' in info else None,
        number_of_employees=int(info.get('Number of employees', '0').replace(',', '') or 0),
        summary=page.summary[:500]
    )
def display(details: InstitutionDetails):
    print(f"Founder: {details.founder or 'N/A'}")
    print(f"Founded: {details.founded or 'N/A'}")
    print(f"Branches: {', '.join(details.branches) if details.branches else 'N/A'}")
    print(f"Employees: {details.number_of_employees or 'N/A'}")
    print(f"Summary: {details.summary or 'N/A'}")
if __name__ == "__main__":
    name = input("Enter institution name: ").strip()
    if name:
        try:
            display(fetch_institution_details(name))
        except ValueError as e:
            print(e)
    else:
        print("Please enter a valid institution name.")

